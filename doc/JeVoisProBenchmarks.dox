/*! \page JeVoisProBenchmarks JeVois-Pro Deep Neural Network Benchmarks


JeVois-Pro neural network backends
==================================

The measurements below were made on a \jvpro smart camera running \jvversion{1.18.0} (September 2022).

- **OpenCV:** network loaded by OpenCV DNN framework and running on CPU.
- **ORT:** network loaded by ONNX Runtime framework and running on CPU.
- **NPU:** network running native on the JeVois-Pro integrated 5-TOPS NPU (neural processing unit).
- **TPU:** network running on the optional 4-TOPS Google Coral TPU accelerator (tensor processing unit).
- **SPU:** network running on the optional 26-TOPS Hailo8 SPU accelerator (stream processing unit).
- **VPU:** network running on the optional 1-TOPS MyriadX VPU accelerator (vector processing unit).
- **NPUX:** network loaded by OpenCV and running on NPU via the TIM-VX OpenCV extension. To run efficiently, network
        should have been quantized to int8, otherwise some slow CPU-based emulation will occur.
- **VPUX:** network optimized for VPU but running on CPU if VPU is not available. Note that VPUX entries are
        automatically created by scanning all VPU entries and changing their target from Myriad to CPU, if a VPU
        accelerator is not detected. If a VPU is detected, then VPU models are listed and VPUX ones are not.
        VPUX emulation runs on the JeVois-Pro CPU using the Arm Compute Library to provide efficient implementation
        of various network layers and operations.

Benchmarking conditions
=======================

- Display was on and 1920x1080/60Hz. Operation is a bit slower if you enable 4K display, likely because of higher
  contension on the memory bus.

- The \jvmod{DNN} module was used, with 1920x1080 YUYV video capture for display purposes, and 1024x576 RGB24 capture
  for vision processing.

- Batch size is always 1, i.e., we measure the round-trip time to pre-process, infer, and post-process one frame at a
  time. Higher performance is usually achieved with larger batch size, but this is not a real-time scenario (would lead
  to larger delays between when a video frame is captured and when the inference results are available and displayed).

- <b>These benchmarks are for JeVois-Pro only and not meant to be representative of a particular accelerator's peak
  performance.</b> In particular:
  
  + The Myriad-X VPU used was a USB dongle connected to JeVois-Pro over a 480 Mbit/s USB 2.0 link. The dongle supports
    5 GBit/s USB 3.0 but the JeVois-Pro CPU has no available USB 3.0 port.
    
  + The NPU is integrated into the Amlogic A311D processor of JeVois-Pro and hence has the highest memory bandwidth
    (direct memory access to the main RAM of the processor), and highest available memory (up to 4 GBytes of main RAM).

  + Coral Edge TPU and Hailo-8 SPU were M.2 2230 A+E cards optionally installed inside JeVois-Pro. Data transfer is
    over PCIe at 5 GBits/s. Note that Hailo-8 can support up to PCIe x4 but the A311D processor of JeVois-Pro only has
    one PCIe x1 lane. Note also that Hailo-8 can support larger PCIe transaction packets (up to 4 Kbytes) than the A311D
    can provide (only up to 256 bytes).

  + Coral Edge TPU has only about 6.5 MBytes of usable RAM on chip. Thus, for larger networks, performance is slower as
    some of the weights may need to constantly be loaded/unloaded over PCIe on every video frame. See, e.g., 45 fps for
    Inception-V3 on 5-TOPS NPU vs. only 21 fps on 4-TOPS TPU, as model size is about 25 MBytes.

  + You can only install one M.2 2230 A+E card inside JeVois-Pro, so you have to choose between a Hailo-8 card, or a
    single-TPU card, or a dual-TPU card (only dual-TPU cards made by JeVois will work; the dual-TPU card made by Google
    requires a PCIe x2 link while JeVois-Pro only has PCIe x1).

  + The \a PreProc time includes resizing the input video (1024x576 RGB24) to the network's input size, and possibly
    swapping RGB/BGR order, NCHW/NHWC order, mean subtraction, normalization by scale factor and/or stdev, and
    quantization to the network's desired data type.

  + The \a Network inference time includes data transfer from main memory to device, on-chip inference, data transfer of
    outputs back to main memory, and possibly dequantization to float32.

  + The \a PostProc time includes decoding of network outputs (e.g., decoding YOLO boxes from raw YOLO layer outputs),
    and drawing results using OpenGL.

Benchmark results
=================

<table>
<tr><th>Pipeline</th><th>Input</th><th>Output</th><th>PreProc</th><th>Network</th><th>PostProc</th><th>Total</th><th>FPS</th></tr>

\htmlinclude benchmarks-1.19.0.html

</table>

Older benchmarks
================

Older benchmarks are provided for comparison as the software evolves over time. Typically, networks running on CPU with
OpenCV backend should get faster over time as more optimized kernels are added to OpenCV. Networks running on hardware
accelerators tend to remain the same. Pre and Post processing are under our control and we strive to make those faster
over time as well, though sometimes adding more features may decrease speed slightly.

- \subpage JeVoisProBenchmarks-1.18.0

*/
