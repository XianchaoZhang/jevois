/*! \page UserDNNtpu 为 Coral TPU 转换并运行神经网络


\jvpro 支持 [Google Coral](https://coral.ai) 4-TOPS 张量处理单元 (TPU) 作为可选的硬件神经加速器。可以使用标准 [Coral M.2 2230 A+E PCIe 板](https://coral.ai/products/m2-accelerator-ae/)、包含 2 个 Coral TPU + 一个 eMMC 闪存盘（安装在单个 M.2 2230 板上）的定制 JeVois 板或任意数量的 [Coral USB 加密狗](https://coral.ai/products/accelerator/)。请注意，与 480 Mbits/s 的 USB 2.0 相比，PCIe 的数据传输速度更快（5 Gbits/s，而 JeVois-Pro 处理器只有一个 5 GBits/s 接口，我们将其用于 PCIe）。

\note 仅限 \jvpro。\jva33 不支持此加速器。

支持的神经网络框架====================================

-TensorFlow / TensorFlow-Lite

TPU 可以运行量化为 int8 权重的模型。它不支持浮点权重，因此需要量化和转换。硬件支持的操作和层类型数量有限，这进一步限制了可以在其上运行的内容。此外，加速器上只有少量 RAM，这进一步限制了可以有效运行的网络规模。但它比标准 CPU 快很多倍。

为了在 TPU 上执行，您的模型将被量化，然后在 Linux 桌面上转换为 blob 格式，然后可以将其传输到 JeVois-Pro microSD 进行执行。

程序 =========

- 阅读并理解有关 \ref UserDNNoverview 的 JeVois 文档

- 确保你理解 \ref UserDNNconv 中的量化概念

- 查看 [官方 Google Coral 文档](https://coral.ai/docs/)

- TPU 仅支持一组特定的层类型。如果您尝试转换包含不受支持的层的网络，转换有时似乎会成功，但转换后的网络可能无法运行，或者使用基于 CPU 的模拟运行非常缓慢。在尝试转换网络之前，请检查 [兼容性概述](https://coral.ai/docs/edgetpu/models-intro/)。特别要注意 Coral 文档中的以下声明：<em>注意：目前，Edge TPU 编译器无法多次对模型进行分区，因此一旦发生不受支持的操作，该操作及其后的所有内容都会在 CPU 上执行，即使稍后发生受支持的操作也是如此。</em>

- 你需要下载并安装 [EdgeTPU 编译器](https://coral.ai/docs/edgetpu/compiler/)，以便在运行 Linux Ubuntu 20.04 的台式计算机上转换/量化你的模型。

- 运行时推理所需的一切（EdgeTPU 运行时库、内核驱动程序、PyCoral）都已预先安装在您的 JeVois microSD 上。

- 获得模型：训练您自己的模型，或下载预先训练的模型。

+ 请注意，TPU 仅有大约 6.5 MB 的板载 RAM 可用于存储模型参数，这充当了伪缓存（请参阅 https://coral.ai/docs/edgetpu/compiler/）。因此，较小的模型可以一次性装入较小的 RAM 中，从而获得最佳性能。较大的模型需要通过 PCIe 或 USB 链路不断加载/卸载权重。较大的模型在 JeVois-Pro 集成 NPU 上运行得更好，它可以直接访问主 RAM（JeVois-Pro 上为 4 GB）。

+ Google 建议从 https://coral.ai/models/ 上的一个模型开始，然后使用你自己的数据对其进行重新训练，如 https://github.com/google-coral/tutorials 中所述

- 获取有关模型的一些参数（例如，预处理平均值、标准差、比例、预期输入图像大小、RGB 或 BGR、打包（NWHC）或平面（NCHW）像素等）。

- 将模型复制到 JEVOIS[PRO]:/share/dnn/custom/ 下的 JeVois microSD 卡

- 为您的模型创建一个 JeVois 模型动物园条目，在其中指定模型参数和复制模型文件的位置。通常，这是 JEVOIS[PRO]:/share/dnn/custom/ 下的 YAML 文件

- 启动 JeVois \jvmod{DNN} 模块。它将扫描自定义目录以查找任何有效的 YAML 文件，并使您的模型作为 DNN 模块的 Pipeline 组件的 \p 管道参数的一个可用值。选择该管道以运行您的模型。

设置 EdgeTPU 编译器 =================================

\note 以下所有内容均应在运行 Ubuntu 20.04 Linux 的快速 x86_64 台式计算机上运行，​​而不是在您的 JeVois-Pro 相机上运行。最后，我们将转换后的模型复制到 microSD，然后使用该模型在 JeVois-Pro 上运行推理。

按照 https://coral.ai/docs/edgetpu/compiler/ 上的说明进行操作

\code{.py} curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

回显“deb https://packages.cloud.google.com/apt coral-edgetpu-stable main”| sudo tee /etc/apt/sources.list.d/coral-edgetpu.list

sudo apt-get 更新

sudo apt-get 安装 edgetpu-编译器

edgetpu_compiler --help \endcode

示例：使用 NASNetMobile 进行对象分类 ====================================================

- 许多预先训练的模型可在 https://coral.ai/models/ 上找到

- 这里，我们使用 NASNetMobile，因为它还不在列表中。

- 让我们尝试使用 NASNetMobile 对 [Coral colab 进行分类模型再训练](https://colab.research.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf2.ipynb) 进行改进。另请查看 [其他 Coral 教程](https://github.com/google-coral/tutorials)。

- 这里我们跳过训练部分，只使用 ImageNet 上的预训练模型，专注于量化和转换到 Edge TPU。

1. 安装 TensorFlow ---------------------

- 首选方法是通过 conda，详情见https://www.tensorflow.org/install/pip

- 在这里，我们将在 python3 虚拟环境中获取 tensorflow wheel，这样步骤更少：

\code{.py} python3 -m venv tf_for_tpu source tf_for_tpu/bin/activate pip install --upgrade pip pip install tensorflow python3 -c "import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))" # 测试安装 \endcode

您可能会看到一些关于缺少 GPU 库的警告，我们在这里忽略这些警告（CPU 足以转换模型），最后是类似 **tf.Tensor(-337.86047, shape=(), dtype=float32)** 的东西，这是我们的测试命令的结果（值 -337.86047 会有所不同，因为它是随机的）。


2. 获取训练好的模型 ------------------------

- 我们在 https://keras.io/api/applications/ 上找到了在 ImageNet 上预先训练的 Keras/Tensorflow NASNetMobile

- 我们将按照 https://keras.io/api/applications/nasnet/#nasnetmobile-function 中的说明将其加载到 TensorFlow 中

- 因此我们开始一个小的**convert.py**脚本，如下所示：\code{.py} import tensorflow as tf import numpy as np

模型 = tf.keras.applications.NASNetMobile()

- 这将使用所有默认值：224x224x3 输入、ImageNet 权重、包括最后的全连接层、包括最终的 softmax 激活。

- 您可以在此阶段重新训练模型。这里我们将按原样使用它。

- 如果我们现在运行**convert.py**，它只会下载模型并退出。

2. 获取量化的样本数据集 ----------------------------------------

- 由于我们使用的是 ImageNet，我们可以从一些内置的 TensorFlow 函数中获取该数据集，但让我们手动执行此操作以查看如何在自定义数据集上执行此操作。

- 我们仍然希望数据能够代表我们的训练数据，因此让我们下载 ImageNet 验证集：

+ 我们访问 https://image-net.org，但即使创建了帐户，也只能通过请求进行下载 
+ 因此，我们从 http://academictorrents.com/details/5d6d0df7ed81efd49ca99ea4737e0ae5e3a5f2e5 获取一个 torrent 文件，并使用“transmission-gtk”（Ubuntu 上预装）下载数据集。

+ 我们获得 ILSVRC2012_img_val.tar，解压后：\code{.py} mkdir dataset cd dataset tar xvf ~/Downloads/ILSVRC2012_img_val.tar cd .. \endcode

+ 我们需要了解预处理的工作原理，以及应该将平均值、标准差和比例应用于原始像素值，以便我们稍后可以设置正确的预处理参数。我们在 [NASNetMobile 的 TensorFlow 文档](https://www.tensorflow.org/api_docs/python/tf/keras/applications/nasnet/preprocess_input) 中找到了一些信息，这些信息表明 nasnet.preprocess_input() 将缩放到 [-1 .. 1]。但没有提到方法......进一步查看[源代码]（https://github.com/keras-team/keras/blob/v2.9.0/keras/applications/nasnet.py#L817-L819），nasnet.preprocess_input（）调用[此处]（https://github.com/keras-team/keras/blob/07e13740fd181fc3ddec7d9a594d8a08666645f6/keras/applications/imagenet_utils.py#L101）定义的 imagenet_utils.preprocess_input（），后者调用定义的 _preprocess_numpy_input（） [这里]（https://github.com/keras-team/keras/blob/07e13740fd181fc3ddec7d9a594d8a08666645f6/keras/applications/imagenet_utils.py#L168）我们最终了解到，在 'tf' 模式下，我们将使用 mean=[127.5 127.5 127.5] 和 scale=1/127.5

+ 我们将以下内容添加到我们的 **convert.py** 中，以我们正在关注的 colab 为蓝本，部分“转换为 TFLite”（我们只需要更改图像文件的位置和预处理）：\code{.py} IMAGE_SIZE = 224

# 提供代表性数据集的生成器 def representative_data_gen(): dataset_list = tf.data.Dataset.list_files('dataset/*') # 修改后的 JEVOIS for i in range(100): image = next(iter(dataset_list)) image = tf.io.read_file(image) image = tf.io.decode_jpeg(image, channels=3) image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE]) image = tf.cast((image - 127.5) / 127.5, tf.float32) # 修改后的 JEVOIS image = tf.expand_dims(image, 0) Yield [image] \endcode

3. 量化模型并转换为 TFLite ----------------------------------------------

我们再次将以下内容添加到我们的 **convert.py** 中，以我们关注的 colab 中的“转换为 TFLite”部分为蓝本：

\code{.py} 转换器 = tf.lite.TFLiteConverter.from_keras_model(模型)

# 这将启用量化转换器。优化 = [tf.lite.Optimize.DEFAULT]

# 这设置了量化的代表性数据集 converter.representative_dataset = representative_data_gen

# 这确保如果任何操作无法量化，转换器就会抛出错误 converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]

# 对于完整的整数量化，虽然支持的类型默认仅为 int8，但为了清楚起见，我们明确声明了它。converter.target_spec.supported_types = [tf.int8]

# 将输入和输出张量设置为 uint8（在 r2.3 中添加）converter.inference_input_type = tf.uint8 converter.inference_output_type = tf.uint8 tflite_model = converter.convert()

使用 open('NASNetMobile_quant.tflite', 'wb') 作为 f: # JEVOIS 修改 f.write(tflite_model) \endcode

我们运行完整的**convert.py**（整理上面的 3 个片段）：

\code{.py} python3 转换.py \endcode

这需要一段时间（也许我们应该安装 GPU 支持），但最终我们得到了 <b>NASNetMobile_quant.tflite</b>，它是我们原始模型的量化版本。

让我们快速检查一下，并将我们的量化模型上传到 Lutz Roeder 的出色 [Netron](https://netron.app/) 在线模型检查工具。上传 **NASNetMobile_quant.tflite** 并检查各个层。特别是，如果您展开任何 Conv 层的输入、权重、偏差和输出详细信息，您将看到数据是如何通过一些相关的量化参数进行 int8 处理的。

4. 将量化的TFLite模型转换为EdgeTPU --------------------------------------------

- 要从量化的 TFLite 转换为 EdgeTPU，我们只需运行：\code{.py} edgetpu_compiler NASNetMobile_quant.tflite \endcode

- 这将移植尽可能多的层和操作以在 TPU 上执行。我们看到这个：\code{.unparsed} Edge TPU 编译器版本 16.0.384591198 启动了 180 秒的编译超时计时器。

模型在 5841 毫秒内成功编译。

输入模型：NASNetMobile_quant.tflite 输入大小：6.21MiB 输出模型：NASNetMobile_quant_edgetpu.tflite 输出大小：8.15MiB 用于缓存模型参数的片上内存：6.31MiB 用于缓存模型参数的剩余片上内存：0.00B 用于流式传输未缓存模型参数的片外内存：635.12KiB Edge TPU 子图数量：1 操作总数：669 操作日志：NASNetMobile_quant_edgetpu.log 有关单个操作的详细信息，请参阅操作日志文件。 编译子进程在超时期限内完成。 编译成功！ \endcode

- 我们得到了 **NASNetMobile_quant_edgetpu.tflite**，我们会将其复制到 JeVois-Pro microSD。\note 有点太大了！从上面的消息来看，我们正在最大限度地利用板载 RAM，除了将图像流式传输到 TPU 之外，每次推理时都需要在该 RAM 和主处理器的 RAM 之间交换 635 KB 的模型参数。

- 我们可以检查生成的 NASNetMobile_quant_edgetpu.log 以确认在这种情况下所有层都已移植到 TPU：\code{.unparsed} Edge TPU 编译器版本 16.0.384591198 输入：NASNetMobile_quant.tflite 输出：NASNetMobile_quant_edgetpu.tflite

操作员计数状态

PAD 20 映射到边缘 TPU ADD 84 映射到边缘 TPU MAX_POOL_2D 4 映射到边缘 TPU MEAN 1 映射到边缘 TPU QUANTIZE 86 映射到边缘 TPU CONV_2D 196 映射到边缘 TPU CONCATENATION 20 映射到边缘 TPU FULLY_CONNECTED 1 映射到边缘 TPU RELU 48 映射到边缘 TPU MUL 4 映射到边缘 TPU SOFTMAX 1 映射到边缘 TPU STRIDED_SLICE 4 映射到边缘 TPU AVERAGE_POOL_2D 40 映射到边缘 TPU DEPTHWISE_CONV_2D 160 映射到边缘 TPU \endcode


5. 为我们的新模型创建一个 zoo YAML 文件 ----------------------------------------------

现在我们需要让 JeVois 了解我们的模型，方法是创建一个描述模型和文件位置的小型 YAML 文件。我们只需从预加载的 JeVois **tpu.yml**（在 GUI 的配置选项卡中）中获取一个条目即可获得灵感，并创建我们的新 **NASNetMobile.yml**：

\code{.py} %YAML 1.0 ---

NASNetMobile：预处理：Blob 网络类型：TPU 后处理：分类模型：“dnn/custom/NASNetMobile_quant_edgetpu.tflite” 强度：“NHWC：8U：1x224x224x3：AA：0.0078125：128” 平均值：“127.5 127.5 127.5” 比例：0.0078125 类：“coral/classification/imagenet_labels.txt” 类别偏移量：1 \endcode

\note 对于 \p 类，我们使用已预加载到 microSD 上的现有 ImageNet 标签文件，因为我们没有从 Keras 获取该文件。由于该标签文件的第一个条目为“背景”，而我们的模型未使用此条目，因此我们使用 \p classoffset 1 来移动类标签。如果标签似乎不正确，您可以在运行时调整它。如果您使用自定义训练的模型，您还应该将文件 **NASNetMobile.labels** 复制到 microSD，该文件描述了您的类名（每行一个类标签），然后将 classes 参数设置为该文件。

6. 复制到 microSD 并运行 --------------------------

- 将 **NASNetMobile_quant_edgetpu.tflite** 和 **NASNetMobile.yml** 复制到 JeVois-Pro microSD 上的 /jevoispro/share/dnn/custom/。

- 启动 \jvmod{DNN} 模块并选择 \p pipe **TPU:Classify:NASNetMobile**

\jvimg{tpu-nasnetmobile.png, 70%}

- 成功了！请注意，此屏幕截图使用的是连接到 USB 2.0 的 TPU，使用 PCIe TPU 板时速度更快。

提示 ====

- 权重大于 6.5 MB 的模型可以在 TPU 上运行得很好，但速度会更慢。缓存对用户完全透明，并且运行良好。

- 在 JeVois-Pro 上，可以同时为多个不同的模型实例化多个 Coral TPU 管道。这些模型将自动且透明地在硬件加速器上进行时间复用。例如，在 JeVois 模块 \jvmod{MultiDNN} 或 \jvmod{MultiDNN2} 中，您可以将多个管道（在模块的 params.cfg 文件中）设置为 TPU 模型，即使您只有一个 TPU，也不会出现任何冲突或问题。

- 如果您有多个 TPU，则可以使用 YAML 参数 **tpunum** 在给定的 TPU 上运行给定的模型。

- 另请参阅 \ref UserDNNtips

*/

