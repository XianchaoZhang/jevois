namespace jevois {
namespace dnn {

/*! \page UserDNNoverview Running neural networks on JeVois-A33 and JeVois-Pro

The JeVois \jvmod{DNN} module provides a generic engine to run neural network inference on \jva33 and \jvpro. Note that
currently no neural network training is available on JeVois yet, as training usually requires large servers with big
GPUs. Thus here we assume that you have an already-trained model which you want to use on your JeVois camera for runtime
inference on live video streams.

While we here focus on the JeVois \jvmod{DNN} module, several older modules provide DNN functionality:

- \jvmod{TensorFlowEasy}: TensorFlow-Lite object classification on CPU using TensorFlow API
- \jvmod{TensorFlowSaliency}: Itti et al. (1998) saliency model + TensorFlow-Lite object classification
  on CPU using TensorFlow API
- \jvmod{TensorFlowSingle}: TensorFlow-Lite object classification on CPU using TensorFlow API
- \jvmod{DarknetSingle}: Darknet object recognition on CPU, using Darknet API
- \jvmod{DarknetSaliency}: Itti et al. (1998) saliency model + Darknet object recognition on CPU, Darknet API
- \jvmod{DarknetYOLO}: Darknet YOLO object detection on CPU, Darknet API
- \jvmod{DetectionDNN}: Object detection using OpenCV on CPU
- \jvmod{PyDetectionDNN}: Object detection using OpenCV on CPU, Python version
- \jvmod{PyClassificationDNN}: Object classification using OpenCV on CPU, Python version
- \jvmod{PyEmotion}: Facial emotion recognition network, in Python

- \jvpro only: \jvmod{PyFaceMesh}: Facial landmarks using MediaPipe
- \jvpro only: \jvmod{PyHandDetector}: Hand landmarks using MediaPipe
- \jvpro only: \jvmod{PyPoseDetector}: Body pose landmarks using MediaPipe

- \jvpro only: \jvmod{MultiDNN}: Run multiple neural networks in parallel, display in quadrants
- \jvpro only: \jvmod{MultiDNN2}: Run multiple neural networks in parallel, overlapped displays
- \jvpro only: \jvmod{PyCoralClassify}: Run classification models on optional Coral TPU, using Coral Python API
- \jvpro only: \jvmod{PyCoralDetect}: Run detection models on optional Coral TPU, using Coral Python API
- \jvpro only: \jvmod{PyCoralSegment}: Run segmentation models on optional Coral TPU, using Coral Python API

\note On \jvpro, some of these modules are under the **Legacy** list of modules in the graphical interface.

JeVois-Pro DNN Benchmarks with various hardware accelerators
============================================================

See \ref JeVoisProBenchmarks

JeVois DNN framework overview
=============================

The \jvmod{DNN} module implements a Pipeline component, which serves as overall inference orchestrator, as well as a
factory for three sub-components:

- PreProcessor: receives an image from the camere sensor and prepares it for network inference (e.g., resize, swap
  RGB to BGR, quantize, etc).\n
  Available variants:
  + \ref PreProcessorBlob (C++): should be appropriate for most networks that expect one image as input
  + write your own in Python following the interface described in PreProcessor, PreProcessorPython,
    and the example in \ref PyPreBlob.py

- Network: receives a pre-processed image and runs neural network inference, producing some outputs.\n
  Available variants:
  + NetworkOpenCV (C++) for OpenCV, OpenVino/Myriad-X, and TIM-VX/NPU
  + NetworkNPU (C++)
  + NetworkHailo (C++)
  + NetworkTPU (C++)
  + NetworkONNX (C++) for ONNX Runtime (also available in Python)
  + write your own in python following the interface in Network, NetworkPython and the example in \ref PyNetOpenCV.py

- PostProcessor: receives the raw network outputs and presents them in a human-friendly way. For example, draw boxes
  on the live camera video after running an object detection network.\n
  Available variants:
  + PostProcessorClassify
  + PostProcessorDetect
  + PostProcessorSegment (semantic segmentation)
  + PostProcessorYuNet (for face detection boxes + markers on eyes, nose, and mouth)
  + PostProcessorStub (useful to test a model for speed before you write your own preprocessor)
  + write your own using the interface in PostProcessor, PostProcessorPython and the example in \ref PyPostClassify.py

The parameters of a Pipeline are specified in a YAML file that describes which pre-processor to use, which network type,
which post-processor, and various parameters for these, as well as where the trained weights are stored on
microSD. These YAML files are stored in JEVOIS[PRO]:/share/dnn/ and available on \jvpro through the Config tab of the
user interface.

A given network is selected in the \jvmod{DNN} module via the \p pipe parameter of the Pipeline component. Available
pipes are described in that parameter as:

\verbatim
<ACCEL>:<TYPE>:<NAME>
\endverbatim

where ACCEL is one of (OpenCV, NPU, SPU, TPU, VPU, NPUX, VPUX, Python), and TYPE is one of (Stub, Classify, Detect,
%Segment, YuNet, Python, Custom).

The following keys are used in the JeVois-Pro GUI (\p pipe parameter of Pipeline component):

- **OpenCV:** network loaded by OpenCV DNN framework and running on CPU.
- **ORT:** network loaded by ONNX Runtime framework and running on CPU.
- **NPU:** network running native on the JeVois-Pro integrated 5-TOPS NPU (neural processing unit).
- **TPU:** network running on the optional 4-TOPS Google Coral TPU accelerator (tensor processing unit).
- **SPU:** network running on the optional 26-TOPS Hailo8 SPU accelerator (stream processing unit).
- **VPU:** network running on the optional 1-TOPS MyriadX VPU accelerator (vector processing unit).
- **NPUX:** network loaded by OpenCV and running on NPU via the TIM-VX OpenCV extension. To run efficiently, network
        should have been quantized to int8, otherwise some slow CPU-based emulation will occur.
- **VPUX:** network optimized for VPU but running on CPU if VPU is not available. Note that VPUX entries are
        automatically created by scanning all VPU entries and changing their target from Myriad to CPU, if a VPU
        accelerator is not detected. If a VPU is detected, then VPU models are listed and VPUX ones are not.
        VPUX emulation runs on the JeVois-Pro CPU using the Arm Compute Library to provide efficient implementation
        of various network layers and operations.
        
For example:

\code{.py}
%YAML 1.0
---

# SqueezeNet v1.1 from https://github.com/DeepScale/SqueezeNet
SqueezeNet:
  preproc: Blob
  nettype: OpenCV
  postproc: Classify
  model: "opencv-dnn/classification/squeezenet_v1.1.caffemodel"
  config: "opencv-dnn/classification/squeezenet_v1.1.prototxt"
  intensors: "NCHW:32F:1x3x227x227"
  mean: "0 0 0"
  scale: 1.0
  rgb: false
  classes: "classification/imagenet_labels.txt"
  classoffset: 1
\endcode

will be available in the \jvmod{DNN} module via the \p pipe parameter of Pipeline as **OpenCV:Classify:SqueezeNet**

For an up-to-date list of supported keys in the YAML file, see all the parameters defined (using
`JEVOIS_DECLARE_PARAMETER(...)`) in:

- \ref PreProcessor.H
- \ref Network.H
- \ref PostProcessor.H
- \ref Pipeline.H

From the links above, click on <em>Go to the source code of this file</em> to see the parameter definitions.

Procedure to add a new network
==============================

- Everything you need at runtime (OpenCV full, with all available backends, targets, etc, OpenVino, Coral EdgeTPU
  libraries, Hailo libraries, NPU libraries, etc) is pre-installed on JeVois, so you do not need to install any
  additional software on the camera to run your custom networks using these frameworks.

- Obtain a model: train your own, or download a pretrained model.

- Obtain some parameters about the model (e.g., pre-processing mean, stdev, scale, expected input image size, RGB or
  BGR, packed (NWHC) or planar (NCHW) pixels, names of the input and output layers, etc).

- For running on \jvpro, convert/quantize the model on your desktop Linux computer, so that it is optimized to run on
  one of the available neural accelerators, like integrated NPU, Hailo8, Coral TPU, etc.
  
  + This will require that you install a vendor-provided SDK for each target accelerator (e.g., Amlogic NPU SDK,
    OpenVino SDK, Hailo SDK, Coral EdgeTPU compiler), on a fast Linux desktop with plenty of RAM, disk space, and
    possibly a big nVidia GPU.
    
  + For quantization, you will also need a <em>representative sample dataset</em>. This usually is about 100 images from
    the validation set used for your model. The goal is to run this dataset through the original network (forward
    inference only) and record the range of values encountered on every layer. These ranges of values will then be used
    to quantize the layers with best accuracy.

  + Using the vendor SDK for the acelerator of your choice, convert and quantize the model on your fast Linux desktop.
  
- Copy model to JeVois microSD card under JEVOIS[PRO]:/share/dnn/custom/

- Create a JeVois model zoo entry for your model, where you specify the model parameters and the location where you
  copied your model files. Typically this is a YAML file under JEVOIS[PRO]:/share/dnn/custom/

- On the camera, launch the JeVois \jvmod{DNN} module. It will scan the custom directory for any valid YAML file, and
  make your model available through the \p pipe parameter of the DNN module's Pipeline component. Select that pipe to
  run your model.

- You can adjust many parameters while the model is running (e.g., confidence threshold, pre-processing mean and scale,
  swap RGB/BGR), while others are frozen at runtime (e.g., input tensor dimensions, post-processor type). Once you
  determine good values for the online-tunable parameters, you can copy those values to your YAML file. Frozen
  parameters can only changed in the YAML file.

Details for the available frameworks
====================================

- \jva33 and \jvpro: \subpage UserDNNconv
- \jva33 and \jvpro: \subpage UserDNNopencv
- \jvpro only: \subpage UserDNNnpu
- \jvpro only: \subpage UserDNNspu
- \jvpro only: \subpage UserDNNtpu
- \jvpro only: \subpage UserDNNvpu
- \subpage UserDNNtips

*/
}
}
