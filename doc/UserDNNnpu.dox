/*! \page UserDNNnpu 为 JeVois-Pro NPU 转换并运行神经网络

\jvpro 包含一个 5-TOPS 神经处理单元 (NPU)，该单元集成在 Amlogic A311D 处理器中。该神经加速器与主内存的连接速度最快（直接 DMA 访问），因此数据传输和网络执行速度非常快。此外，它不会像其他一些加速器那样受到片上内存有限的困扰，因此相当大的网络可以在 NPU 上运行。

\note 仅限 \jvpro。\jva33 不支持此加速器。

支持的神经网络框架====================================

- Caffe
- TensorFlow
- TensorFlow-Lite
- ONNX（以及通过转换为 ONNX 的 pyTorch）
- Darknet
- Keras

NPU 可以运行量化为 uint8、int8 或 int16 权重的模型。它不支持浮点权重，因此需要量化和转换。硬件支持的操作和层类型数量有限，这进一步限制了可以在其上运行的内容。但它比标准 CPU 快很多倍。

为了在 NPU 上执行，您的模型将被量化，然后在 Linux 桌面上转换为专有 blob 格式，然后可以将其传输到 JeVois-Pro microSD 进行执行。

程序 =========

- 阅读并理解有关 \ref UserDNNoverview 的 JeVois 文档

- 确保你理解 \ref UserDNNconv 中的量化概念

- 查看 [NPU SDK 文档](https://github.com/khadas/aml_npu_sdk/tree/master/docs)。特别值得关注的是 [模型转码和运行用户指南](https://github.com/khadas/aml_npu_sdk/blob/master/docs/)。

- NPU 仅支持一组特定的层类型。如果您尝试转换包含不受支持的层的网络，转换有时可能看似成功，但转换后的网络可能无法运行。在尝试转换网络之前，请先查看 [层和操作支持指南](https://github.com/khadas/aml_npu_sdk/blob/master/docs/)。

- 您需要下载并安装 Amlogic NPU SDK 才能在运行 Linux Ubuntu 20.04 的台式计算机上转换/量化您的模型。

- 运行时所需的一切（NPU 运行时库）都已预先安装在您的 JeVois-Pro microSD 上。

- 获得模型：训练您自己的模型，或下载预先训练的模型。

- 获取有关模型的一些参数（例如，预处理平均值、标准差、比例、预期输入图像大小、RGB 或 BGR、打包（NWHC）或平面（NCHW）像素等）。

- 将模型复制到 JEVOIS[PRO]:/share/dnn/custom/ 下的 JeVois-Pro microSD 卡

- 为您的模型创建一个 JeVois 模型动物园条目，在其中指定模型参数和复制模型文件的位置。通常，这是 JEVOIS[PRO]:/share/dnn/custom/ 下的 YAML 文件

- 启动 JeVois \jvmod{DNN} 模块。它将扫描自定义目录以查找任何有效的 YAML 文件，并使您的模型作为 DNN 模块的 Pipeline 组件的 \p 管道参数的一个可用值。选择该管道以运行您的模型。

设置 NPU SDK ========================

\note 以下所有内容均应在运行 Ubuntu 20.04 Linux 的快速 x86_64 台式计算机上运行，​​而不是在您的 JeVois-Pro 相机上运行。最后，我们将转换后的模型复制到 microSD，然后使用该模型在 JeVois-Pro 上运行推理。

Amlogic/VeriSilicon NPU SDK 由 Khadas 分发，该公司制造使用与 \jvpro 相同的 Amlogic A311D 处理器的开发板。

\code{.py} git lfs clone --recursive https://github.com/khadas/aml_npu_sdk.git cd aml_npu_sdk/acuity-toolkit/demo \endcode

无需安装 Ubuntu 包或 Python wheels，所有内容都包含在该 git repo 中。

其中有 3 个脚本需要编辑，然后在桌面上按顺序运行：

- **0_import_model.sh**：从源框架（Caffe、TensorFlow 等）转换为中间表示，然后计算样本数据集上每一层遇到的值范围的一些统计数据。这些值范围将用于设置量化参数。

- **1_quantize_model.sh**：使用非对称仿射 uint8 或动态定点 int8 或 int16 量化模型。这将生成我们将在 \jvpro 上运行的模型，采用 <b>.nb</b> 专有二进制 blob 格式。

- **2_export_case_code.sh**：创建一些可以在目标平台（如 JeVois-Pro）上编译的 C 代码，以创建一个独立的应用程序，该应用程序将在一个图像上加载和运行模型。我们不会使用该代码，因为 JeVois 软件提供了自己的代码，可将模型直接链接到相机传感器和 GUI。但是，我们将对其进行检查，以便我们可以获取 YAML zoo 文件的输入和输出规范。

对于第 2 步，我们需要一个有代表性的样本数据集，通常是来自训练或验证集的约 100 张图像。这非常重要，因为它将设置量化参数。

示例：使用 YOLOv7-tiny 进行对象检测 =============================================

我们选择 YOLOv7-tiny 作为本教程的原因是：

- 它以 Darknet 格式提供，仅使用 NPU 支持的操作和层
- JeVois 已经为其提供了后处理器

1. 获取模型 ----------------

- 前往 https://github.com/AlexeyAB/darknet

- 让我们下载在 416x416 输入上运行的 YOLOv7-tiny。单击最新版本，然后单击底部资产列表中的 \b yolov7-tiny.weights，或者运行以下命令：\code{.py} wget https://github.com/AlexeyAB/darknet/releases/download/yolov4/yolov7-tiny.weights \endcode

- 我们还需要网络结构的描述，我们可以在 repo 的 **cfg** 文件夹中找到它：\code{.py} wget https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov7-tiny.cfg \endcode

2. 获取样本数据集 -----------------------

- 此模型在 [COCO 数据集](https://cocodataset.org) 上进行训练。让我们下载 [2017 年验证集](http://images.cocodataset.org/zips/val2017.zip)：\code{.py} wget http://images.cocodataset.org/zips/val2017.zip unzip val2017.zip \endcode

- 有 5,000 张图像，比我们需要的多。unix 命令“shuf”可以随机打乱名称列表并取前 \p n，因此让我们使用它从数据集中抓取 100 张随机图像。我们将这些文件路径的列表保存到 <b>dataset.txt</b>，供 **0_import_model.sh** 使用：\code{.py} ls ./val2017/*.jpg | shuf -n 100 > dataset.txt \endcode

- **dataset.txt** 现在应包含 100 行（由于 shuf 是随机的，因此您的结果会有所不同）：\code{.unparsed} ./val2017/000000382030.jpg ./val2017/000000436617.jpg ./val2017/000000138550.jpg ./val2017/000000226154.jpg ./val2017/000000319369.jpg ... \endcode

3.编辑并运行0_import_model.sh ---------------------------------

- 该文件包含各种框架的各种注释示例。在这里我们需要：
+ 更改名称
+ 启用 Darknet 简介，注释掉其他框架。注意：pegasus 运行两次，第一次用于导入，然后生成有关输入的元数据。我们只用我们要从中导入的框架替换第一个。

+（Darknet 不需要，其他可能需要）将输入大小列表设置为我们的输入大小，根据 **yolov7-tiny.cfg**，这里是 1x3x416x416。
+我们需要知道输入预处理比例、平均值、标准差。Tiny YOLO v7 只期望像素值在 [0.0 .. 1.0] 内的 RGB 图像。因此我们将使用平均值 = [0 0 0]、标准差 = [1 1 1]、比例 = 1/255 = 0.0039215686、rgb = true。

+ 下面，**channel-mean-value** 需要 4 个值：3 个颜色通道的 3 个均值和 1 个比例。

- 我们最终得到这个修改后的 **0_import_model.sh**:\code{.py} #!/bin/bash

NAME=yolov7-tiny # JEVOIS 编辑了 ACUITY_PATH=../bin/

pegasus=${ACUITY_PATH}pegasus 如果 [ !-e "$pegasus" ]; 然后 pegasus=${ACUITY_PATH}pegasus.py fi

$pegasus 导入 darknet\ --模型 ${NAME}.cfg \ --weights ${NAME}.weights \ --输出模型 ${NAME}.json \ --输出数据 ${NAME}.data \ $pegasus 生成输入元数据 \ --模型 ${NAME}.json \ --输入元数据输出 ${NAME}_inputmeta.yml \ --通道平均值“0 0 0 0.0039215686” \ # JEVOIS 编辑 --源文件数据集.txt \endcode

\note 请勿剪切和粘贴上面的“# JEVOIS 编辑”注释，它们会破坏代码，请将其删除。

- 运行它，它应该完成并且没有错误：\code{.py} ./0_import_model.sh \endcode

- 您可以查看 yolov7-tiny_inputmeta.yml 了解输入参数的描述，查看 yolov7-tiny.json 了解模型图的描述。

- 如果此步骤出现错误，则可能是某些操作不受支持，这可能发生在网络末端。在这种情况下，请检查 Netron 中的网络并查看哪一层转换失败，然后添加 **--outputs /some/layer** 以将网络截断到该层。

4.编辑并运行1_quantize_model.sh -----------------------------------

- 因为我们的输入范围是[0.0 .. 1.0]，这需要 uint8 非对称仿射量化，以便我们的预处理和随后的量化将减少为无操作（参见 \ref UserDNNconv）。

- 因此我们启用该功能并更改模型名称，最终得到修改后的 **1_quantize_model.sh**:\code{.py} #!/bin/bash

NAME=yolov7-tiny # JEVOIS 编辑了 ACUITY_PATH=../bin/

pegasus=${ACUITY_PATH}pegasus 如果 [ !-e "$pegasus" ]; 然后 pegasus=${ACUITY_PATH}pegasus.py fi

$pegasus quantize \ --quantizer asymmetric_affine \ # JEVOIS 编辑 --qtype uint8 \ # JEVOIS 编辑 --rebuild \ --with-input-meta ${NAME}_inputmeta.yml \ --model ${NAME}.json \ --model-data ${NAME}.data \endcode

- 运行它，它应该会顺利完成：\code{.py} ./1_quantize_model.sh \endcode

- 您可以查看 yolov7-tiny.quantize 以查看样本数据集上每个层的最小/最大值范围，以及每个层如何相应地量化。下一个脚本运行时，此文件将被删除。

5. 编辑并运行2_export_case_code.sh -------------------------------------

- 在这里，我们只需要设置模型名称，并选择正确的 NPU 模型，对于 \jvpro 中的 A311D 处理器，其为 **VIPNANOQI_PID0X88**。我们最终得到：\code{.py} #!/bin/bash

NAME=yolov7-tiny # JEVOIS 编辑了 ACUITY_PATH=../bin/

pegasus=$ACUITY_PATH/pegasus 如果 [ !-e "$pegasus" ]; 然后 pegasus=$ACUITY_PATH/pegasus.py fi

$pegasus 导出 ovxlib \ --model ${NAME}.json \ --model-data ${NAME}.data \ --model-quantize ${NAME}.quantize \ --with-input-meta ${NAME}_inputmeta.yml \ --dtype quantized \ --optimize VIPNANOQI_PID0X88 \ # JEVOIS 编辑 --viv-sdk ${ACUITY_PATH}vcmdtools \ --pack-nbg-unify

rm -rf ${名称}_nbg_unify

mv../*_nbg_unify${名称}_nbg_unify

cd ${名称}_nbg_unify

mv network_binary.nb ${NAME}.nb

光盘 ..

# 保存正常情况演示导出.数据 mkdir -p ${NAME}_normal_case_demo mv *.h *.c .project .cproject *.vcxproj BUILD *.linux *.export.data ${NAME}_normal_case_demo

# 删除 normal_case 演示源 #rm *.h *.c .project .cproject *.vcxproj BUILD *.linux *.export.data

rm *.数据 *.量化 *.json *_inputmeta.yml \endcode

- 运行它，它应该完成并且没有错误：\code{.py} ./2_export_case_code.sh \endcode

- 好的，我们需要的一切都在 yolov7-tiny_nbg_unify/
+ 转换后的模型：yolov7-tiny.nb
+ C 代码：vnn_yolov7tiny.c，我们将检查它以得出我们的输入和输出张量规范。

6. 创建 YAML zoo 文件 -----------------------

- 我们从 JeVois microSD 中已有的 zoo 文件 npu.yml 中的 **YoloV4** 条目开始（并且可以在 GUI 的 Config 选项卡中使用）。

- 为了获取量化输入和输出的规格，我们检查 yolov7-tiny_nbg_unify/vnn_yolov7tiny.c 并查找张量定义。我们找到了这个（添加了注释来解释下一步）：\code /*----------------------------------------- Tensor Initialize -----------------------------------------*/ attr.dtype.fmt = VSI_NN_DIM_FMT_NCHW; /* @input_0:out0 */ attr.size[0] = 416; // JEVOIS：最后一个维度（变化最快；此处为 W）attr.size[1] = 416; // JEVOIS：下一个维度（此处为 H）attr.size[2] = 3; // JEVOIS：下一个维度（此处为 C）attr.size[3] = 1; // JEVOIS：第一个维度（此处为 N）attr.dim_num = 4; // JEVOIS：输入应为 4D 张量 attr.dtype.scale = 0.003921568393707275; // JEVOIS：AA 量化的比例 attr.dtype.zero_point = 0; // JEVOIS：AA 量化的零点 attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC; // JEVOIS：即 AA 量化 NEW_NORM_TENSOR(norm_tensor[0], attr, VSI_NN_TYPE_UINT8); // JEVOIS：即 8U 类型

/* @output_90_198:out0 */ attr.size[0] = 52; attr.size[1] = 52; attr.size[2] = 255; attr.size[3] = 1; attr.dim_num = 4; attr.dtype.scale = 0.0038335032295435667; attr.dtype.zero_point = 0; attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC; NEW_NORM_TENSOR(norm_tensor[1], attr, VSI_NN_TYPE_UINT8);

/* @output_94_205:out0 */ attr.size[0] = 26; attr.size[1] = 26; attr.size[2] = 255; attr.size[3] = 1; attr.dim_num = 4; attr.dtype.scale = 0.0038371747359633446; attr.dtype.zero_point = 0; attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC; NEW_NORM_TENSOR(norm_tensor[2], attr, VSI_NN_TYPE_UINT8);

/* @output_98_212:out0 */ attr.size[0] = 13; attr.size[1] = 13; attr.size[2] = 255; attr.size[3] = 1; attr.dim_num = 4; attr.dtype.scale = 0.003918845672160387; attr.dtype.zero_point = 0; attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC; NEW_NORM_TENSOR(norm_tensor[3], attr, VSI_NN_TYPE_UINT8);

因此，我们有一个输入和三个输出（针对 3 个 YOLO 尺度），并且我们从上面生成的代码中得出它们的 JeVois 规格，如下所示（参见 \ref UserDNNconv）：\code{.py} intensors：“NCHW：8U：1x3x416x416：AA：0.003921568393707275：0” outtensors：“8U：1x255x52x52：AA：0.0038335032295435667：0， 8U：1x255x26x26：AA：0.0038371747359633446：0， 8U：1x255x13x13：AA：0.003918845672160387：0” \endcode

- 对于 YOLO 后处理，我们需要锚点的定义。我们从 yolov7-tiny.cfg 中获取这些内容：\code{.py} anchors = 10,13, 16,30, 33,23, 30,61, 62,45, 59,119, 116,90, 156,198, 373,326 \endcode 在下面的 YAML 文件中，我们将按 YOLO 尺度拆分此列表（此处，9 个 w,h 对对应 3 个尺度中的 3 对。我们在下面用分号分隔尺度）。

- 由于 YOLOv7 使用“新样式”的 YOLO 坐标，我们需要禁用后处理器 sigmoid 并将后处理器 scalexy 设置为 2.0。对于 YOLOv5/v7，您可能希望这样做，并将 sigmoid 设置为 true 并将 scalexy 设置为 0.0，以便为 YOLOv2/v3/v4 使用旧样式的框坐标。您可以在 jevois::dnn::PostProcessorDetectYOLO::yolo_one() 中查看差异

- 我们只需修改名称和文件位置，将所有全局定义放入我们的文件中（例如 preproc、nettype 等，它们在 npu.yml 中全局设置，因此对于我们正在复制的 YoloV4 条目不会重复），并最终得到以下 **yolov7-tiny.yml**（预处理器平均值和比例与我们在 0_import_model.sh 中使用的一样）：\code{.py} %YAML 1.0 ---

yolov7-tiny： preproc：Blob 平均值：“0 0 0” 比例：0.0039215686 nettype：NPU 模型：“dnn/custom/yolov7-tiny.nb” 张力：“NCHW：8U：1x3x416x416：AA：0.003921568393707275：0” 输出张力：“8U：1x255x52x52：AA：0.0038335032295435667：0，8U：1x255x26x26：AA：0.0038371747359633446：0，8U：1x255x13x13：AA：0.003918845672160387：0” postproc：检测 检测类型： RAWYOLO 类：“npu/detection/coco-labels.txt” 锚点：“10,13, 16,30, 33,23；30,61, 62,45, 59,119；116,90, 156,198, 373,326” sigmoid：false scalexy：2.0 \endcode

- 我们将 yolov7-tiny.yml 和 yolov7-tiny.nb 复制到 JeVois-Pro 上的 /jevoispro/share/dnn/custom/ 然后尝试一下！

7. 测试模型并调整任何参数 -------------------------------------------

- 选择 \jvmod{DNN} 机器视觉模块

- 将 \p 管道参数设置为 **NPU:Detect:yolov7-tiny**

\jvimg{yolov7tiny-npu.png, 70%}

成功了！速度也相当快，仅网络推理（在 NPU 上运行的部分）就约为 55 fps。

\note 您可以在 **1_quantize_model.sh** 中使用 int8-DFP 量化重复本教程。然后，您只需将 YAML 中的 intensors 和 outtensors 规范更改为获得的 DFP 参数。我们做到了，网络运行良好，但速度大约减半（约 22.5 fps）。因此，AA 量化是此网络的正确选择。

提示 ====

- 如果您在尝试运行网络时收到“图形验证失败”提示，则可能是您输入的张量规格不正确。在 GUI 的“配置”选项卡下，您可以编辑 yolov7-tiny.yml 并修复它。

- Khadas 有自己的文档，您可能想查看一下，因为他们的 VIM3 板使用与 JeVois-Pro 相同的 Amlogic A311D 处理器。但请注意，在 JeVois 上，我们跳过使用生成的 C 代码，因为 JeVois-Pro 已经提供了运行 NPU 模型所需的所有代码。
+ https://docs.khadas.com/linux/vim3/NPUSDK.html
+ https://docs.khadas.com/linux/vim3/ConvertToUseNPU.html
+ https://docs.khadas.com/linux/vim3/NPUPerformanceUsage.html
+ https://docs.khadas.com/linux/vim3/NPUOperationTimes.html

- 从 pyTorch 转换时，如果您收到一些奇怪的错误，例如 \verbatim RuntimeError: [enforce fail at inline_container.cc:208] . file not found: archive/constants.pkl \endverbatim 您的模型可能是使用比 NPU SDK 中包含的版本更新的 pyTorch 版本保存的。您可能需要将模型保存到 ONNX，然后尝试再次从 ONNX 转换。或者可能是模型使用了 NPU 不支持的层类型或操作，在这种情况下转换为 ONNX 将无济于事。您需要将网络更改为仅包含可以映射到 NPU 的层和操作。

\note 到目前为止，我们还无法使用 NPU SDK 直接从 pyTorch 成功转换，我们总是会遇到某种错误。但首先将源模型导出到 ONNX，然后在其上运行 NPU SDK 就可以正常工作，只要在源模型中仅使用 NPU 支持的操作即可。

- 有关我们有时如何跳过不受支持的最后一层（例如，执行重塑、检测框解码、非最大框抑制等）的示例，请参阅 \ref UserDNNvpu 和 \ref UserDNNspu。

- 另请参阅 \ref UserDNNtips

另一个示例：使用 YOLOv10n 进行对象检测 ==================================================

- YOLOv10n 的流程几乎相同。我们必须对一些事情进行微调：

- 从 https://github.com/THU-MIG/yolov10/tree/main 获取代码

- 完成其安装步骤，包括创建 conda 环境和获取所有依赖项。

- 您还可以在此阶段使用自定义数据集进行重新训练。

- 要导出到 ONNX，使用 opset 13 会出现一些转换错误，因此我们使用 opset 12。我们还可以在该步骤中使用“imgsz”参数设置自定义图像分辨率：\code{.py} yolo export model=jameslahm/yolov10n format=onnx opset=12 simply imgsz=288,512 \endcode

- 网络末端的一些层出现大小错误，可能是因为不同的 opset。无论如何，我们想要原始的 YOLO 输出。因此，我们在 Netron 中检查了网络，并决定在 **0_import_model.sh** 中使用 `/model.23/Transpose_output_0` 作为输出层：\code{.py}
#...#Onnx $pegasus import onnx\ --model ${NAME}.onnx \ --output-model ${NAME}.json \ --outputs /model.23/Transpose_output_0 \ --output-data ${NAME}.data
#...\endcode

- 转换正常。但是，它没有在显示屏上生成任何框。检查输出张量后，所有类的置信度值始终为零。查看转换过程中生成的 yolov10n-512x288.quantize 显示输出张量中的值范围为 [-108.24 .. 611.29]。实际上，在该输出张量中，对于输入 1x3x288x512，其为 1x3024x84，4 个框坐标（可以在 512x288 内变化，如果框部分超出输入图像，则变化更多）和 80 个类置信度（在 [0..1] 中）被连接在一起。这意味着，使用 8 位量化，[0..1] 中的任何类置信度将始终映射到相同的数字（量化的零点）...

- 因此我们将量化改为 dynamic_fixed_point 和 int16。结果使用 5 位表示小数部分，这足以合理地表示类别准确度。

- 效果很好，并且 NPU 的 YOLOv10n 现在包含在 JeVois 发行版和 microSD 图像中。 */

